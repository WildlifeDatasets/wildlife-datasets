{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539571f6",
   "metadata": {},
   "source": [
    "# WildlifeReID-10k creation - part 2\n",
    "\n",
    "This is the second part for creating the WildlifeReID-10k dataset. It creates the split for the dataset obtained in the first part.\n",
    "\n",
    "First load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from wildlife_datasets import datasets, splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8adf3",
   "metadata": {},
   "source": [
    "We specify the roots, load the dataset and features and verify whether the orderings of the dataset and the features correspond to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/data/wildlife_datasets/data/WildlifeReID10k'\n",
    "save_clusters_prefix = 'clusters/cluster'\n",
    "os.makedirs(save_clusters_prefix, exist_ok=True)\n",
    "d = datasets.WildlifeReID10k(root)\n",
    "df = d.df\n",
    "if not np.array_equal(df.index, range(len(df))):\n",
    "    raise Exception('Index must be 0..n')\n",
    "for name, df_dataset in df.groupby('dataset'):\n",
    "    features_names = np.load(f'features/names_{name}.npy', allow_pickle=True)\n",
    "    if not np.array_equal(df_dataset['path'], features_names):\n",
    "        raise Exception('Features were computed for different indices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224e70d",
   "metadata": {},
   "source": [
    "We create the splits dataset-by-dataset. We first use `OpenSetSplit` from `wildlife-datasets`, which creates an open-set split with approximately 80% images in the training size. Approximately 10% of images depict new individuals: they are only in the testing but not in the training set. We then repslit the open-set split using `resplit_by_features`. For each individual, it keeps the same number of images in the training and testing set but reshuffles them. It shuffles them based so that the images whose `features` are similar, end up all in the training set. This prevents the information leak from the training to the testing set when images in both sets are similar or even the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2967f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df_dataset in df.groupby('dataset'):\n",
    "    print(name)\n",
    "    features = np.load(f'features/features_{name}.npy')\n",
    "    for i in range(len(features)):\n",
    "        features[i] /= np.linalg.norm(features[i])\n",
    "    splitter = splits.OpenSetSplit(0.8, 0.1, seed=666)    \n",
    "    idx_train0, idx_test0 = splitter.split(df_dataset)[0]\n",
    "    idx_train, idx_test = splitter.resplit_by_features(df_dataset, features, idx_train0, save_clusters_prefix=save_clusters_prefix)\n",
    "\n",
    "    df.loc[idx_train, 'split'] = 'train'\n",
    "    df.loc[idx_test, 'split'] = 'test'\n",
    "df = df.drop('image_id', axis=1)\n",
    "df.to_csv(os.path.join(root, 'metadata.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
