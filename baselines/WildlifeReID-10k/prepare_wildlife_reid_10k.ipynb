{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3c62d9",
   "metadata": {},
   "source": [
    "# WildlifeReID-10k creation - part 1\n",
    "\n",
    "This is the first part for creating the WildlifeReID-10k dataset. It copies the files to a separate folder, applies bounding boxes and masks and combined them together. The split is created in the second part.\n",
    "\n",
    "First load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d83e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from wildlife_datasets.preparation import prepare_functions, species_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b64d16",
   "metadata": {},
   "source": [
    "Then specify the roots, where the dataset is located. Parameters `size` specifies the size to which the datasets will be resized and parameters `copy_files` whether the files copied from `root_datasets` to `root`. Since bounding boxes and masks are applied and the black borders are cropped, it is relatively time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d562243",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_datasets = '/data/wildlife_datasets/data'\n",
    "root = os.path.join(root_datasets, 'WildlifeReID10k')\n",
    "root_images = os.path.join(root, 'images')\n",
    "root_metadata = os.path.join(root, 'metadata')\n",
    "transform = None\n",
    "copy_files = True\n",
    "names_permissible = [\n",
    "    'AAUZebraFish',\n",
    "    'AerialCattle2017',\n",
    "    'AmvrakikosTurtles',\n",
    "    'ATRW',\n",
    "    'BelugaID',\n",
    "    'BirdIndividualID',\n",
    "    'CatIndividualImages',\n",
    "    'CTai',\n",
    "    'CZoo',\n",
    "    'Chicks4FreeID',\n",
    "    'CowDataset',\n",
    "    'Cows2021',\n",
    "    'DogFaceNet',\n",
    "    'FriesianCattle2015',\n",
    "    'FriesianCattle2017',\n",
    "    'GiraffeZebraID',\n",
    "    'Giraffes',\n",
    "    'HyenaID2022',\n",
    "    'IPanda50',\n",
    "    'LeopardID2022',\n",
    "    'MPDD',\n",
    "    'MultiCamCows2024',\n",
    "    'NDD20',\n",
    "    'NyalaData',\n",
    "    'OpenCows2020',\n",
    "    'PolarBearVidID',\n",
    "    'PrimFace',\n",
    "    'ReunionTurtles',\n",
    "    'SealID',\n",
    "    'SeaStarReID2023',\n",
    "    'SeaTurtleID2022',\n",
    "    'SMALST',\n",
    "    'SouthernProvinceTurtles',\n",
    "    'StripeSpotter',\n",
    "    'WhaleSharkID',\n",
    "    'ZakynthosTurtles',\n",
    "    'ZindiTurtleRecall',\n",
    "]\n",
    "remove_str = ['[', ']']\n",
    "replace_extensions = {'.webp': '.jpg'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c12480",
   "metadata": {},
   "source": [
    "Create metadata for each dataset and potentially copy the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, prepare in prepare_functions.items():\n",
    "    if name in names_permissible:\n",
    "        print(name)\n",
    "        os.makedirs(f'{root_metadata}/{name}/', exist_ok=True)\n",
    "        metadata_part = prepare(f'{root_datasets}/{name}', f'{root_images}/{name}', transform=transform, copy_files=copy_files, remove_str=remove_str, replace_extensions=replace_extensions)\n",
    "        metadata_part.to_csv(f'{root_metadata}/{name}/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc548d",
   "metadata": {},
   "source": [
    "The next codes adds additional information to the metadata and combines them together. After this code, the dataset is finished but splits. To compute splits, we first need to compute the features from `extract_features.py` and then compute the actual splits in `prepare_wildlife_reid_10k_2.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcdd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for name in prepare_functions:\n",
    "    if name in names_permissible:\n",
    "        metadata_part = pd.read_csv(f'{root_metadata}/{name}/metadata.csv')\n",
    "        metadata_part['dataset'] = name\n",
    "        metadata_part['identity'] = name + '_' + metadata_part['identity'].astype(str)\n",
    "        metadata_part['path'] = 'images/' + name + '/' + metadata_part['path']\n",
    "        metadata_part['species'] = metadata_part['species'].apply(lambda x: species_conversion[x])\n",
    "        metadata.append(metadata_part)\n",
    "metadata = pd.concat(metadata).reset_index(drop=True)\n",
    "metadata = metadata.drop('image_id', axis=1)\n",
    "metadata['image_id'] = range(len(metadata))\n",
    "idx = ~metadata['date'].isnull()\n",
    "idx = metadata.index[idx]\n",
    "metadata.loc[idx, 'date'] = pd.to_datetime(metadata.loc[idx, 'date'].astype(str).apply(lambda x: x[:10]), format='%Y-%m-%d').astype(str)\n",
    "metadata['orientation'] = metadata['orientation'].replace({'below': 'down', 'up': 'top', 'above': 'top'})\n",
    "metadata.to_csv(f'{root}/metadata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
