{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3c62d9",
   "metadata": {},
   "source": [
    "# WildlifeReID-10k creation\n",
    "\n",
    "This is the notebook for creating the WildlifeReID-10k dataset. It copies the files to a separate folder, applies bounding boxes and masks and combines them together. It alsi creates the splits. All these operations are created dataset-wise.\n",
    "\n",
    "First load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from thresholds import names_thresholds\n",
    "from utils import SplitterByFeatures\n",
    "\n",
    "from wildlife_datasets.datasets import WildlifeReID10k\n",
    "from wildlife_datasets.preparation import prepare_functions, species_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b64d16",
   "metadata": {},
   "source": [
    "Then specify the roots, where the dataset is located. Parameters `transform` can be used to resize files, parameter `copy_files` whether the files copied from `root_datasets` to `root` and finally `add_split` whether split should be added. Since bounding boxes and masks are applied and the black borders are cropped, it is relatively time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562243",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_datasets = \"/data/wildlife_datasets/data\"\n",
    "root = os.path.join(root_datasets, \"WildlifeReID10k\")\n",
    "root_images = os.path.join(root, \"images\")\n",
    "root_metadata = os.path.join(root, \"metadata\")\n",
    "root_clusters = \"clusters\"\n",
    "root_features = \"features_dino\"\n",
    "os.makedirs(root_clusters, exist_ok=True)\n",
    "\n",
    "transform = None\n",
    "copy_files = False\n",
    "add_split = True\n",
    "\n",
    "names_permissible = list(names_thresholds.keys())\n",
    "remove_str = [\"[\", \"]\"]\n",
    "replace_extensions = {\".webp\": \".jpg\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c12480",
   "metadata": {},
   "source": [
    "Create metadata for each dataset and potentially copy the files. The structure is probably a bit wrong because the notebook needs to be first run with `copy_files=True` and `add_split=False`, then the features need to be computed by the script `extract_features.py` and then the boolean parameters need to be reverted to add splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, prepare in prepare_functions.items():\n",
    "    if name in names_permissible:\n",
    "        print(name)\n",
    "        os.makedirs(f\"{root_metadata}/{name}/\", exist_ok=True)\n",
    "        thr, splitter = names_thresholds[name]\n",
    "        if thr != \"time-aware\" and thr is not None:\n",
    "            path_features = f\"{root_features}/features_{name}.npy\"\n",
    "            path_clusters = f\"{root_clusters}/clusters_{name}_{thr}.npy\"\n",
    "            splitter = SplitterByFeatures(path_features, splitter, thr, file_name=path_clusters)\n",
    "        metadata_part = prepare(\n",
    "            f\"{root_datasets}/{name}\",\n",
    "            f\"{root_images}/{name}\",\n",
    "            transform=transform,\n",
    "            add_split=add_split,\n",
    "            splitter=splitter,\n",
    "            copy_files=copy_files,\n",
    "            remove_str=remove_str,\n",
    "            replace_extensions=replace_extensions,\n",
    "        )\n",
    "        metadata_part.to_csv(f\"{root_metadata}/{name}/metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc548d",
   "metadata": {},
   "source": [
    "The next codes adds additional information to the metadata and combines them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcdd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for name in prepare_functions:\n",
    "    if name in names_permissible:\n",
    "        metadata_part = pd.read_csv(f\"{root_metadata}/{name}/metadata.csv\")\n",
    "        metadata_part[\"dataset\"] = name\n",
    "        metadata_part[\"identity\"] = name + \"_\" + metadata_part[\"identity\"].astype(str)\n",
    "        metadata_part[\"path\"] = \"images/\" + name + \"/\" + metadata_part[\"path\"]\n",
    "        metadata_part[\"species\"] = metadata_part[\"species\"].apply(lambda x: species_conversion[x])\n",
    "\n",
    "        thr, _ = names_thresholds[name]\n",
    "        metadata_part[\"cluster_id\"] = pd.Series(dtype=object)\n",
    "        if thr != \"time-aware\" and thr is not None:\n",
    "            path_clusters = f\"{root_clusters}/clusters_{name}_{thr}.npy\"\n",
    "            if os.path.exists(path_clusters):\n",
    "                clusters = np.load(path_clusters)\n",
    "                metadata_part[\"cluster_id\"] = clusters\n",
    "                metadata_part[\"cluster_id\"] = metadata_part[\"cluster_id\"].astype(object)\n",
    "        elif thr == \"time-aware\":\n",
    "            for i, (_, metadata_date) in enumerate(metadata_part.groupby([\"identity\", \"date\"])):\n",
    "                metadata_part.loc[metadata_date.index, \"cluster_id\"] = str(i)\n",
    "        idx = ~metadata_part[\"cluster_id\"].isnull()\n",
    "        metadata_part.loc[idx, \"cluster_id\"] = (\n",
    "            metadata_part.loc[idx, \"identity\"] + \"_\" + metadata_part.loc[idx, \"cluster_id\"].astype(int).astype(str)\n",
    "        )\n",
    "\n",
    "        metadata.append(metadata_part)\n",
    "metadata = pd.concat(metadata).reset_index(drop=True)\n",
    "metadata = metadata.drop(\"image_id\", axis=1)\n",
    "idx = ~metadata[\"date\"].isnull()\n",
    "idx = metadata.index[idx]\n",
    "metadata.loc[idx, \"date\"] = pd.to_datetime(\n",
    "    metadata.loc[idx, \"date\"].astype(str).apply(lambda x: x[:10]), format=\"%Y-%m-%d\"\n",
    ").astype(str)\n",
    "metadata[\"orientation\"] = metadata[\"orientation\"].replace({\"below\": \"down\", \"up\": \"top\", \"above\": \"top\"})\n",
    "metadata.to_csv(f\"{root}/metadata.csv\", index=False)\n",
    "\n",
    "dataset = WildlifeReID10k(root)\n",
    "dataset.df = dataset.df.drop(\"image_id\", axis=1)\n",
    "dataset.df.to_csv(f\"{root}/metadata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
